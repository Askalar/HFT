{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pingouin as pg\n",
    "import datetime\n",
    "import seaborn as sns \n",
    "import tensorflow as tf\n",
    "import time\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import zscore\n",
    "from arch.unitroot import engle_granger\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from metric_clf_hft import metric\n",
    "from sklearn import feature_selection\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.datasets import make_classification\n",
    "from lightgbm import LGBMClassifier\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "\n",
    "\n",
    "pd.options.mode.chained_assignment = None \n",
    "pd.set_option('display.max_rows', 100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions for features calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Order_book_spread(data: pd.DataFrame):\n",
    "    \"\"\"book\"\"\"\n",
    "    #1\n",
    "    data['Book_spread'] = (data['asks[0].price'] -data['bids[0].price'])\n",
    "\n",
    "def Trade_price_spread(data: pd.DataFrame, delta_t: str='1000ms'):\n",
    "    \"\"\"\"trades\"\"\"\n",
    "    #checked\n",
    "    if 'timestamp' not in data.columns:\n",
    "        data['timestamp'] = pd.to_datetime(data.index, unit='ns')\n",
    "    High = data[['price','timestamp']].rolling(delta_t, on='timestamp', min_periods=1).max()\n",
    "    Low = data[['price','timestamp']].rolling(delta_t, on='timestamp', min_periods=1).min()\n",
    "    data['price_spread_'+delta_t] = High.price - Low.price\n",
    "    del High, Low\n",
    "    \n",
    "def Trade_price_speed(data: pd.DataFrame, delta_t: str='1000ms'):\n",
    "    \"\"\"\"trades\"\"\"\n",
    "    #checked\n",
    "    if 'price_spread_' + delta_t not in data.columns:\n",
    "        Trade_price_spread(data, delta_t)\n",
    "    delta_t_int = int(delta_t[:-2])\n",
    "    data['price_speed_'+delta_t] = data['price_spread_'+delta_t]/delta_t_int\n",
    "\n",
    "def Trades_frequency(data: pd.DataFrame, delta_t: str='500ms'):\n",
    "    \"\"\"\"trades\"\"\"\n",
    "    #checked\n",
    "    if 'timestamp' not in data.columns:\n",
    "        data['timestamp'] = pd.to_datetime(data.index, unit='ns')\n",
    "    delta_t_int = int(delta_t[:-2])\n",
    "    data['Trades_frequency'] = data[['price','timestamp']].rolling(delta_t, on='timestamp', min_periods=1).count().price/delta_t_int   \n",
    "\n",
    "def Trades_volume_speed(data: pd.DataFrame, delta_t: str='500ms'):\n",
    "    \"\"\"\"trades\"\"\"\n",
    "    #checked    \n",
    "    if 'timestamp' not in data.columns:\n",
    "        data['timestamp'] = pd.to_datetime(data.index, unit='ns')\n",
    "    delta_t_int = int(delta_t[:-2])\n",
    "    data['Trades_volume_speed'] = data[['amount','timestamp']].rolling(delta_t, on='timestamp', min_periods=1).sum().amount/delta_t_int\n",
    "\n",
    "    data['sells_amount'] = np.where(data.side == 'S', data['amount'], 0)\n",
    "    data['buys_amount'] = np.where(data.side == 'B', data['amount'], 0)\n",
    "\n",
    "    data['Sells_volume_speed'] = data[['sells_amount','timestamp']].rolling(delta_t, on='timestamp', min_periods=1).sum().sells_amount/delta_t_int\n",
    "    data['Buys_volume_speed'] = data[['buys_amount','timestamp']].rolling(delta_t, on='timestamp', min_periods=1).sum().buys_amount/delta_t_int\n",
    "\n",
    "    del data['buys_amount'], data['sells_amount']\n",
    "\n",
    "\n",
    "def Big_trades_frequency(data: pd.DataFrame, delta_t: str='5000ms', delta_t_block: str='50ms', times_mean: int=3):\n",
    "    \"\"\"\"trades\"\"\"\n",
    "    #checked \n",
    "    if 'timestamp' not in data.columns:\n",
    "        data['timestamp'] = pd.to_datetime(data.index, unit='ns')\n",
    "    delta_t_int = int(delta_t[:-2])\n",
    "    data['block_amount'] =  data[['amount','timestamp']].rolling(delta_t_block, on='timestamp', min_periods=1).sum().amount\n",
    "    data['C_amount'] = data['block_amount'].cumsum()\n",
    "    data['one'] = 1\n",
    "    data['C_count'] = data.one.cumsum()\n",
    "    data['block_amount_mean'] = data['C_amount']/data['C_count']\n",
    "    del data['C_amount'], data['one'], data['C_count']\n",
    "    data['big_trade_flg'] = np.where(data['block_amount'] > data['block_amount_mean'] * times_mean, 1, 0)\n",
    "    data['Big_trades_frequency'] = data[['big_trade_flg','timestamp']].rolling(delta_t, on='timestamp', min_periods=1).sum().big_trade_flg/delta_t_int\n",
    "\n",
    "    data['big_sells_amount'] = np.where((data['block_amount'] > data['block_amount_mean'] * times_mean) & (data.side == 'S'), data['amount'], 0)\n",
    "    data['big_buys_amount'] = np.where((data['block_amount'] > data['block_amount_mean'] * times_mean) & (data.side == 'B'), data['amount'], 0)\n",
    "\n",
    "    data['Big_sells_amount_speed'] = data[['big_sells_amount','timestamp']].rolling(delta_t, on='timestamp', min_periods=1).sum().big_sells_amount/delta_t_int\n",
    "    data['Big_buys_amount_speed'] = data[['big_buys_amount','timestamp']].rolling(delta_t, on='timestamp', min_periods=1).sum().big_buys_amount/delta_t_int\n",
    "\n",
    "    del data['big_trade_flg'], data['big_sells_amount'], data['big_buys_amount'], data['block_amount_mean']\n",
    "\n",
    "\n",
    "def Book_sum_volume_speed(data: pd.DataFrame, delta_t: str='1000ms', n_levels: int=10) -> np.float64:\n",
    "    \"\"\"\"book\"\"\"\n",
    "    #checked \n",
    "    if 'timestamp' not in data.columns:\n",
    "        data['timestamp'] = pd.to_datetime(data.index, unit='ns')\n",
    "    data['asks_volume'] = 0\n",
    "    data['bids_volume'] = 0\n",
    "    for i in range(n_levels):\n",
    "        data['asks_volume'] = data['asks['+str(i)+'].amount'] + data['asks_volume'] \n",
    "        data['bids_volume'] = data['bids['+str(i)+'].amount'] + data['bids_volume']\n",
    "    delta_t_int = int(delta_t[:-2])\n",
    "    data_prev = data[['timestamp','asks_volume','bids_volume']]\n",
    "    data_prev.timestamp = data_prev.timestamp + pd.to_timedelta(delta_t) \n",
    "    data_delta = pd.merge_asof(data[['timestamp','asks_volume','bids_volume']], data_prev[['timestamp','asks_volume','bids_volume']], suffixes=[None, '_prev'] , direction='backward', left_on='timestamp' , right_on='timestamp')\n",
    "    data['book_ask_volume_speed'] = (data_delta['asks_volume'].values - data_delta['asks_volume_prev'].values)/delta_t_int\n",
    "    data['book_bid_volume_speed'] = (data_delta['bids_volume'].values - data_delta['bids_volume_prev'].values)/delta_t_int\n",
    "    data['book_ask_volume_speed'] = data['book_ask_volume_speed'].fillna(0)\n",
    "    data['book_bid_volume_speed'] = data['book_bid_volume_speed'].fillna(0)\n",
    "\n",
    "\n",
    "def Delta_volume_without_trades(book: pd.DataFrame, trades: pd.DataFrame, n_levels: int=9) -> np.float64:\n",
    "    \"\"\"\"book and trades\"\"\"\n",
    "    #checked \n",
    "    if 'timestamp' not in book.columns:\n",
    "        book['timestamp'] = pd.to_datetime(book.index, unit='ns')\n",
    "    if 'timestamp' not in trades.columns:\n",
    "        trades['timestamp'] = pd.to_datetime(trades.index, unit='ns')\n",
    "    if 'Sells_volume_speed'not in trades.columns:\n",
    "        Trades_volume_speed(trades)\n",
    "    if 'book_ask_volume_speed' not in book.columns:\n",
    "        Book_sum_volume_speed(book)  \n",
    "    data = pd.merge_asof(book[['timestamp', 'book_ask_volume_speed','book_bid_volume_speed']], trades[['timestamp', 'Sells_volume_speed', 'Buys_volume_speed']], suffixes=[None, '_2'] , direction='backward', left_on='timestamp' , right_on='timestamp')\n",
    "    data = data.set_index('timestamp')\n",
    "    data = data.sort_values(by=['timestamp'])\n",
    "    book['Delta_ask_without_trades'] = data['book_ask_volume_speed'].values - data['Buys_volume_speed'].values\n",
    "    book['Delta_ask_without_trades'] = book['Delta_ask_without_trades'].fillna(0) \n",
    "    book['Delta_bid_without_trades'] = data['book_bid_volume_speed'].values - data['Sells_volume_speed'].values\n",
    "    book['Delta_bid_without_trades'] = book['Delta_bid_without_trades'].fillna(0) \n",
    "    del data\n",
    "\n",
    "\n",
    "def Naive_delays(book: pd.DataFrame, delta_t: str='10000ms'):\n",
    "    \"\"\"book\"\"\"\n",
    "    #checked\n",
    "    if 'timestamp' not in book.columns:\n",
    "        book['timestamp'] = pd.to_datetime(book.index, unit='ns')\n",
    "    delta_t_int = int(delta_t[:-2])\n",
    "    book_prev = book[['timestamp','asks[0].amount','bids[0].amount']]\n",
    "    book_prev['timestamp'] = book_prev['timestamp'] + pd.to_timedelta(delta_t) \n",
    "\n",
    "    data_delta = pd.merge_asof(book[['timestamp','asks[0].amount','bids[0].amount']], \n",
    "                               book_prev[['timestamp','asks[0].amount','bids[0].amount']], \n",
    "                               suffixes=[None, '_prev'] , direction='backward', \n",
    "                               left_on='timestamp' , right_on='timestamp')\n",
    "    data_delta['one'] = 1\n",
    "    data_delta['delta_ask_amount'] = abs(data_delta['asks[0].amount'] - data_delta['asks[0].amount_prev'])\n",
    "    data_delta['delta_bid_amount'] = abs(data_delta['bids[0].amount'] - data_delta['bids[0].amount_prev'])\n",
    "    \n",
    "    data_delta['mean_delta_ask_amount'] = data_delta['delta_ask_amount'].cumsum()/data_delta['one'].cumsum()\n",
    "    data_delta['mean_delta_bid_amount'] = data_delta['delta_bid_amount'].cumsum()/data_delta['one'].cumsum()\n",
    "\n",
    "    book['asks_delay'] = book['asks[0].amount'] / (data_delta['mean_delta_ask_amount'].values/delta_t_int)\n",
    "    book.asks_delay.replace([np.inf, -np.inf], [100000, 0], inplace=True)\n",
    "    book['asks_delay'] = book['asks_delay'].fillna(10000)\n",
    "    book['bids_delay'] = book['bids[0].amount'] / (data_delta['mean_delta_bid_amount'].values/delta_t_int)\n",
    "    book.bids_delay.replace([np.inf, -np.inf], [100000, 0], inplace=True)\n",
    "    book['bids_delay'] = book['bids_delay'].fillna(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Average_directional_index(data: pd.DataFrame, delta_n: int=50, period: int=14):\n",
    "    #checked, no value in predictions\n",
    "    data['high'] = data.old_price.rolling(delta_n, min_periods=1).max()\n",
    "    data.high.fillna(data.old_price)\n",
    "    data['low'] = data.old_price.rolling(delta_n, min_periods=1).min()\n",
    "    data.low.fillna(data.old_price)\n",
    "    data['+DM'] = data.high - data.high.shift(delta_n)\n",
    "    data['+DM'].fillna(0)\n",
    "    data['-DM'] = data.low.shift(delta_n) - data.low \n",
    "    data['-DM'].fillna(0)\n",
    "    data['H-L'] = data.high - data.low\n",
    "    data['H-CL'] = np.where(abs(data.high - data.old_price.shift(delta_n)) > 0,\n",
    "                            abs(data.high - data.old_price.shift(delta_n)), 0.0)\n",
    "    data['L-CL'] = np.where(abs(data.low - data.old_price.shift(delta_n)) > 0,\n",
    "                            abs(data.low - data.old_price.shift(delta_n)), 0.0)   \n",
    "    data['TR'] = data[['H-L', 'H-CL', 'L-CL']].max(axis=1)\n",
    "    data['TR'].fillna(1)\n",
    "    del data['H-L'], data['H-CL'], data['L-CL'], data['low'], data['high']\n",
    "    data['ATR'] = data.TR.ewm(span= delta_n * period).mean()\n",
    "    data['ATR'].fillna(1)\n",
    "    del data['TR']  \n",
    "    data['+DIA'] = 100 * data['+DM'].ewm(span= delta_n * period).mean()/data['ATR']\n",
    "    data['-DIA'] = 100 * data['-DM'].ewm(span= delta_n * period).mean()/data['ATR']\n",
    "    data['DIA'] = abs(abs(data['+DIA'])-abs(data['-DIA'])) / abs(abs(data['+DIA']) + abs(data['-DIA'])) * 100\n",
    "    data['ADX'] = data['DIA'].ewm(alpha=1/period, adjust=False).mean()\n",
    "    data['ADX'] = data['ADX'].fillna(17)\n",
    "    data['+DIA'] = data['+DIA'].fillna(17)\n",
    "    data['-DIA'] = data['-DIA'].fillna(17)\n",
    "    del data['DIA']\n",
    "\n",
    "    \n",
    "def Chande_momentum_oscillator(data: pd.DataFrame, delta_n: int=10, period: int=30):\n",
    "    #checked but now worth\n",
    "    roll = pd.concat([data['old_price'].shift(i).rename(i) for i in range(0, delta_n*period, delta_n)], axis=1)\n",
    "    data['Su'] = roll.lt(data.old_price, axis=0).sum(axis=1)\n",
    "    data['Sd'] = roll.gt(data.old_price, axis=0).sum(axis=1)\n",
    "    data['CMO'] = 100 * (data.Su - data.Sd)/(data.Su + data.Sd)\n",
    "    data['CMO'] = data.CMO.fillna(0)\n",
    "    del data['Sd'], data['Su'], roll\n",
    "\n",
    "\n",
    "def Momentum(data: pd.DataFrame, delta_n: int=100, period: int=1):\n",
    "    # checked.?\n",
    "    data['MOM'] = data.price - data.price.shift(delta_n*period)\n",
    "    data['MOM'] = data.MOM.fillna(0)\n",
    "\n",
    "\n",
    "def Rate_of_change(data: pd.DataFrame, delta_n: int=10, period: int=10):\n",
    "    #checked, no value\n",
    "    data['ROC'] = (data.old_price/data.old_price.shift(delta_n*period) - 1) * 100\n",
    "    data['ROC'] = data.ROC.fillna(0)\n",
    "\n",
    "\n",
    "def Relative_strength_index(data: pd.DataFrame, delta_n: int=10, period: int=14):\n",
    "    #checked?\n",
    "    data['CLd'] = data.old_price - data.old_price.shift(delta_n)\n",
    "    data['CLd'] = data['CLd'].fillna(0)\n",
    "    data['CLdd'] = data.CLd - data.CLd.shift(delta_n)\n",
    "    data['CLdd'] = data['CLdd'].fillna(0)\n",
    "    data['CL_plus'] = np.select([data.CLdd > 0], [abs(data.CLd)], default=0)\n",
    "    data['CL_minus'] = np.select([data.CLdd < 0], [abs(data.CLd)], default=0)\n",
    "    data['CL_plus'] = data.CL_plus.rolling(delta_n*period, min_periods=1).sum()\n",
    "    data['CL_minus'] = data.CL_minus.rolling(delta_n*period, min_periods=1).sum()\n",
    "    data['RSI'] = 100 - 100 / (1 + data.CL_plus/data.CL_minus)\n",
    "    data.RSI.replace([np.inf, -np.inf], [100, 0], inplace=True)\n",
    "    data['RSI'] = data.RSI.fillna(50)\n",
    "    del data['CLd'], data['CL_plus'], data['CL_minus']\n",
    "\n",
    "\n",
    "def Stochast_relative_strength_index(data: pd.DataFrame, delta_n: int=10, period: int=20):\n",
    "    #checked?\n",
    "    Relative_strength_index(data, delta_n, period)\n",
    "    data['HRSI'] = data.RSI.rolling(delta_n*period, min_periods=1).max()\n",
    "    data['LRSI'] = data.RSI.rolling(delta_n*period, min_periods=1).min()\n",
    "    data['StochRSI'] = (data.RSI - data.LRSI) / (data.HRSI - data.LRSI) * 100\n",
    "    data.StochRSI.replace([np.inf, -np.inf], [100, 0], inplace=True)\n",
    "    data['StochRSI'] = data.StochRSI.fillna(50)\n",
    "    del data['LRSI'], data['HRSI']\n",
    "\n",
    "\n",
    "def Linear_regression_line(data: pd.DataFrame, delta_n: int=10, period: int=10):\n",
    "    #checked?no need\n",
    "    data['x2'] = data.old_price * data.old_price\n",
    "    data['y'] = data.old_price.shift(-delta_n)\n",
    "    data['y'] = data['y'].fillna(data.old_price)\n",
    "    data['xy'] = data.old_price * data['y'] \n",
    "    data['LRLa'] = (delta_n * data['xy'].rolling(delta_n, min_periods=1).sum() - data.old_price.rolling(delta_n, min_periods=1).sum() * data['y'].rolling(delta_n, min_periods=1).sum())\\\n",
    "                / (delta_n * data['x2'].rolling(delta_n, min_periods=1).sum() - data.old_price.rolling(delta_n, min_periods=1).sum() * data.old_price.rolling(delta_n, min_periods=1).sum())\n",
    "    data['LRLa'].replace([np.inf, -np.inf], [1, 1], inplace=True) \n",
    "    data['LRLa'] = data['LRLa'].fillna(1)\n",
    "    data['b'] = (data['y'].rolling(delta_n, min_periods=1).sum()  - data.LRLa * data.old_price.rolling(delta_n, min_periods=1).sum()) / delta_n\n",
    "    data['LRL'] = data['LRLa'] * data.old_price + data['b']\n",
    "    data['LRL'] = data.LRL.fillna(0)\n",
    "    del data['xy'], data['x2'], data['y']\n",
    "\n",
    "\n",
    "def Realized_Volatility(data: pd.DataFrame, delta_n: int=10, period: int=10):\n",
    "    #checked\n",
    "    data['log_return'] = np.log(data['price']).diff()\n",
    "    data['real_variance'] = data['log_return'].rolling(delta_n*period, min_periods=1).var() #* annuity_factor\n",
    "    data['R_Volatility'] = np.sqrt(data['real_variance'])\n",
    "    data['R_Volatility'] = data.R_Volatility.fillna(0)\n",
    "    del data['log_return'], data['real_variance']\n",
    "\n",
    "\n",
    "def Gauss_Kernel(data: pd.DataFrame, delta_n: int=10, std: np.float64=0.02):\n",
    "    #checked\n",
    "    data['Kernel'] = data.price.rolling(window=delta_n, win_type='gaussian', min_periods=1).mean(std = std)\n",
    "    data['Kernel'] = data['Kernel'].fillna(data.price)\n",
    "\n",
    "\n",
    "def Autocorrelation(data: pd.DataFrame, delta_n: int=30, period: int=4, corr_in_range: bool=True):\n",
    "    #checked but no value\n",
    "    if corr_in_range:\n",
    "        for period_i in range(1, period + 1):\n",
    "            data['Autocorrelation_lag_' + str(period_i)] = data.price.rolling(delta_n*period_i, min_periods=1).corr(data.price.shift(delta_n*period_i))   \n",
    "            data['Autocorrelation_lag_' + str(period_i)] = data['Autocorrelation_lag_' + str(period_i)].fillna(0)\n",
    "            data['Autocorrelation_lag_' + str(period_i)].replace([np.inf, -np.inf], [1, -1], inplace=True)\n",
    "    else:\n",
    "        data['Autocorrelation_lag_' + str(period)] = data.price.rolling(delta_n*period, min_periods=1).corr(data.price.shift(delta_n*period))\n",
    "        data['Autocorrelation_lag_' + str(period)] = data['Autocorrelation_lag_' + str(period)].fillna(0)\n",
    "        data['Autocorrelation_lag_' + str(period)].replace([np.inf, -np.inf], [1, -1], inplace=True)\n",
    "\n",
    "\n",
    "def Partial_correlation(data: pd.DataFrame, delta_n: int=10, period: int=2):\n",
    "    #checked but no value    \n",
    "    if 'Autocorrelation_lag_1' not in data.columns:\n",
    "        Autocorrelation(data, delta_n, period=1)\n",
    "    if 'Autocorrelation_lag_' + str(period) not in data.columns:\n",
    "        Autocorrelation(data, delta_n, period=period)\n",
    "    data['r23'] = data.Autocorrelation_lag_1.rolling(delta_n, min_periods=1).corr(data['Autocorrelation_lag_' + str(period)])\n",
    "    data['r23'] = data['r23'].fillna(0)\n",
    "    data['r23'].replace([np.inf, -np.inf], [1, -1], inplace=True)\n",
    "    data['Partial_correlation_'+str(period)] = (data['Autocorrelation_lag_' + str(period)] - data.r23 * data.Autocorrelation_lag_1)\\\n",
    "                                                /np.sqrt((1 - np.square(data['Autocorrelation_lag_' + str(period)])) * (1 - np.square(data.Autocorrelation_lag_1)))\n",
    "    data['Partial_correlation_'+str(period)] = data['r23'].fillna(0)\n",
    "    data['Partial_correlation_'+str(period)].replace([np.inf, -np.inf], [1, -1], inplace=True)\n",
    "    del data['r23']\n",
    "\n",
    "\n",
    "def Jump_Variation(data: pd.DataFrame, delta_n: int=30):\n",
    "    #checked\n",
    "    data['square_delta'] = np.square(abs(data.price - data.price.shift(delta_n)))\n",
    "    data['bi_delta'] = np.pi*0.5*(abs(data.price - data.price.shift(delta_n))*abs(data.price.shift(delta_n) - data.price.shift(2*delta_n))\\\n",
    "                        + abs(data.price.shift(delta_n) - data.price.shift(2*delta_n)) *abs(data.price.shift(2*delta_n) - data.price.shift(3*delta_n)))\n",
    "    data['Jump_Variation'] = np.where(data.square_delta - data.bi_delta > 0,\n",
    "                            data.square_delta - data.bi_delta, 0.0)\n",
    "    del data['square_delta'] , data['bi_delta']\n",
    "    data['Jump_Variation'] = data.Jump_Variation.fillna(0)\n",
    "\n",
    "\n",
    "def Cointegration(data: pd.DataFrame, skip_step: int=100):\n",
    "    EG = engle_granger(data['asks[0].price'].iloc[::skip_step], data['bids[0].price'].iloc[::skip_step])\n",
    "    data['Cointegration'] = data['asks[0].price'] + EG.cointegrating_vector['const']  + EG.cointegrating_vector['bids[0].price'] * data['bids[0].price']\n",
    "    return EG\n",
    "\n",
    "\n",
    "def Cointegration_from_EG(data: pd.DataFrame, EG):\n",
    "    data['Cointegration'] = data['asks[0].price'] + EG.cointegrating_vector['const']  + EG.cointegrating_vector['bids[0].price'] * data['bids[0].price']\n",
    "\n",
    "\n",
    "def Order_book_imbalance_level(data: pd.DataFrame, level: int=0):\n",
    "    #checked\n",
    "    data['Book_imb_level_'+ str(level)] = (data['bids['+str(level)+'].amount'] - data['asks['+str(level)+'].amount'])\\\n",
    "                                            /(data['bids['+str(level)+'].amount'] + data['asks['+str(level)+'].amount'])\n",
    "\n",
    "\n",
    "def Trade_imbalance(data: pd.DataFrame, delta_t: str='500ms') -> list:\n",
    "    #checked\n",
    "    if 'timestamp' not in data.columns:\n",
    "        data['timestamp'] = pd.to_datetime(data.index, unit='ns')\n",
    "    data['price_volume'] = np.select([data.side == 'S', data.side == 'B'], \n",
    "                                     [ -data.old_price * data.amount, data.old_price * data.amount], \n",
    "                                     default=0)\n",
    "    TI = data[['price','timestamp']].rolling(delta_t, on='timestamp', min_periods=1).sum()\n",
    "    data['Trade_imbalance'] = TI.price\n",
    "    del data['price_volume']\n",
    "\n",
    "\n",
    "def past_returns(data: pd.DataFrame, delta_n: int=30, period: int=20, delta_t: str='50ms') -> list:\n",
    "    #checked\n",
    "    if 'timestamp' not in data.columns:\n",
    "        data['timestamp'] = pd.to_datetime(data.index, unit='ns')\n",
    "    data['PV'] = data.old_price * data.amount\n",
    "    TI = data[['PV','amount','timestamp']].rolling(delta_t, on='timestamp', min_periods=1).sum() \n",
    "    TI['avg_p'] =  TI.PV/TI.amount\n",
    "    data['past_returns'] = (TI['avg_p']/TI['avg_p'].shift(delta_n*period) - 1)*10000\n",
    "    data['past_returns'] = data['past_returns'].fillna(0)\n",
    "    del TI\n",
    "\n",
    "\n",
    "def Divergence(data_1st_market: pd.DataFrame, data_2nd_market: pd.DataFrame, delta_t: str='10s'): \n",
    "    #may be add weighted price?\n",
    "    if 'timestamp' not in data_1st_market.columns:\n",
    "        data_1st_market['timestamp'] = pd.to_datetime(data_1st_market.index, unit='ns')\n",
    "    if 'timestamp' not in data_2nd_market.columns:\n",
    "        data_2nd_market['timestamp'] = pd.to_datetime(data_2nd_market.index, unit='ns')\n",
    "    data = pd.merge_asof(data_1st_market[['timestamp', 'old_price' ]], data_2nd_market[['timestamp', 'old_price']], suffixes=[None, '_2'] , direction='backward', left_on='timestamp' , right_on='timestamp')\n",
    "    data = data.set_index('timestamp')\n",
    "    data = data.sort_values(by=['timestamp'])\n",
    "    data['d_p1_p2'] = (data.old_price/data.old_price_2 - 1) * 10000 \n",
    "    data['DIV'] = data['d_p1_p2'] - data['d_p1_p2'].rolling(delta_t, min_periods=1).mean()\n",
    "    data_1st_market['DIV'] = data['DIV'].values\n",
    "    data_1st_market['DIV'] = data_1st_market['DIV'].fillna(0)\n",
    "    del data\n",
    "    data = pd.merge_asof(data_2nd_market[['timestamp', 'old_price']], data_1st_market[['timestamp', 'old_price' ]], suffixes=[None, '_2'] , direction='backward', left_on='timestamp' , right_on='timestamp')\n",
    "    data = data.set_index('timestamp')\n",
    "    data = data.sort_values(by=['timestamp'])\n",
    "    data['d_p1_p2'] = (data.old_price/data.old_price_2 - 1) * 10000 \n",
    "    data['DIV'] = data['d_p1_p2'] - data['d_p1_p2'].rolling(delta_t, min_periods=1).mean()\n",
    "    data_2nd_market['DIV'] = data['DIV'].values\n",
    "    data_2nd_market['DIV'] = data_2nd_market['DIV'].fillna(0)\n",
    "    del data\n",
    "\n",
    "\n",
    "def Accumulation_Distribution_Line(data: pd.DataFrame, delta_t: str='1s') -> list:\n",
    "    \"\"\"On trades dataset; delta_t - length of data's blocks\"\"\"\n",
    "    if 'timestamp' not in data.columns:\n",
    "        data['timestamp'] = pd.to_datetime(data.index, unit='ns')\n",
    "    time_data = data[['price','timestamp']].rolling(delta_t, on='timestamp', min_periods=1).min()\n",
    "    data['low'] = time_data.price\n",
    "    time_data = data[['price','timestamp']].rolling(delta_t, on='timestamp', min_periods=1).max()\n",
    "    data['high'] = time_data.price\n",
    "    data['Money_Flow_Multiplier'] = ((data.price - data.low) - (data.high - data.price))/(data.high - data.low)\n",
    "    del data['high'], data['low']\n",
    "    time_data = data[['amount','timestamp']].rolling(delta_t, on='timestamp', min_periods=1).sum()\n",
    "    data['Money_flow_volume'] = time_data.amount * data['Money_Flow_Multiplier']\n",
    "    data['Money_flow_volume'] = data['Money_flow_volume'].fillna(0)\n",
    "    data['ADL'] = data['Money_flow_volume'].cumsum()\n",
    "    del data['Money_flow_volume'], data['Money_Flow_Multiplier']\n",
    "\n",
    "\n",
    "def Adaptive_log_regression(data: pd.DataFrame, n_levels: int=1, skip_step: int=100):\n",
    "    \"\"\"On merged target with bookdata; n_levels - number of calculated levels of book\"\"\"\n",
    "    #checked\n",
    "    level_names = []\n",
    "    for i in range(n_levels):\n",
    "        level_names.append('asks['+str(i)+'].price') \n",
    "        level_names.append('asks['+str(i)+'].amount')\n",
    "        level_names.append('bids['+str(i)+'].price')\n",
    "        level_names.append('bids['+str(i)+'].amount')\n",
    "    logreg = LogisticRegression(solver='newton-cg', multi_class='multinomial')\n",
    "    logreg.fit(data[level_names].iloc[::skip_step],data.target.iloc[::skip_step])\n",
    "    return logreg, level_names\n",
    "    \n",
    "\n",
    "def IMB(data: pd.DataFrame, n_levels: int=5) -> np.float64:\n",
    "    #checked\n",
    "    data['PVa'] = 0\n",
    "    data['Va'] = 0\n",
    "    data['PVb'] = 0\n",
    "    data['Vb'] = 0\n",
    "    for i in range(n_levels):\n",
    "        data['PVa'] = data['asks['+str(i)+'].price'] * data['asks['+str(i)+'].amount'] + data['PVa']\n",
    "        data['Va'] = data['asks['+str(i)+'].amount'] + data['Va'] \n",
    "        data['PVb'] = data['bids['+str(i)+'].price'] * data['bids['+str(i)+'].amount'] + data['PVb']\n",
    "        data['Vb'] = data['bids['+str(i)+'].amount'] + data['Vb']  \n",
    "    data['IMBa_'+str(n_levels)] = ( (data['PVa']/ data['Va']) /data['asks[0].price'] - 1) * 10000\n",
    "    data['IMBb_'+str(n_levels)] = ( data['bids[0].price'] / (data['PVb']/ data['Vb'])  - 1) * 10000  \n",
    "    data['IMB_'+str(n_levels)] = data['IMBa_'+str(n_levels)] - data['IMBb_'+str(n_levels)]\n",
    "    del data['Va'], data['Vb']\n",
    "\n",
    "\n",
    "def IMB_bp(data: pd.DataFrame, n_levels: int=5) -> np.float64:\n",
    "    #checked\n",
    "    bp = 0.0001\n",
    "    data['PVa'] = 0\n",
    "    data['Va'] = 0\n",
    "    data['PVb'] = 0\n",
    "    data['Vb'] = 0\n",
    "    for i in range(n_levels):\n",
    "        data['PVa'] = data['asks['+str(i)+'].price'] * data['asks['+str(i)+'].amount'] + data['PVa']\n",
    "        data['Va'] = data['asks['+str(i)+'].amount'] + data['Va']\n",
    "     \n",
    "        data['PVb'] = data['bids['+str(i)+'].price'] * data['bids['+str(i)+'].amount'] + data['PVb']\n",
    "        data['Vb'] = data['bids['+str(i)+'].amount'] + data['Vb']  \n",
    "    data['IMBa_'+str(n_levels)] = ( (data['PVa']/ data['Va']) /data['asks[0].price'] - 1) * 10000\n",
    "    data['IMBb_'+str(n_levels)] = ( data['bids[0].price'] / (data['PVb']/ data['Vb'])  - 1) * 10000  \n",
    "    data['IMB_'+str(n_levels)] = data['IMBa_'+str(n_levels)] - data['IMBb_'+str(n_levels)]\n",
    "    del data['Va'], data['Vb']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Create_candles(data_target: pd.DataFrame, data_trades: pd.DataFrame, delta_t: str='15s') -> pd.DataFrame:\n",
    "    delta_t_int = int(delta_t[:-1]) * int(1e9)\n",
    "    timestamps = [i for i in range(data_target.index.min() // delta_t_int * delta_t_int + delta_t_int,\n",
    "                                   data_target.index.max() // delta_t_int * delta_t_int + 2 * delta_t_int, delta_t_int)]\n",
    "    timestamps = pd.DataFrame({'time_point_ns': timestamps})\n",
    "    timestamps.set_index('time_point_ns', drop=True, inplace=True)\n",
    "    timestamps['timestamp'] = pd.to_datetime(timestamps.index, unit='ns')\n",
    "    resu = pd.merge_asof(timestamps , data_trades[['price', 'amount', 'side']], direction='backward', left_on='time_point_ns' , right_on= 'local_ts').dropna()\n",
    "    resu['high'] = resu[['price', 'timestamp']].rolling('1ms', on='timestamp', center=True, min_periods=1).max().price\n",
    "    resu['low'] = resu[['price', 'timestamp']].rolling('1ms', on='timestamp', center=True, min_periods=1).min().price\n",
    "    resu['spread'] = resu.high - resu.low\n",
    "    resu['sell_volume'] = np.select([resu.side == 'S', resu.side == 'B'], \n",
    "                                 [resu.price * resu.amount, 0], \n",
    "                                 default=0)\n",
    "    resu['buy_volume'] = np.select([resu.side == 'S', resu.side == 'B'], \n",
    "                                 [0, resu.price * resu.amount], \n",
    "                                 default=0)\n",
    "    resu['d_volume'] = np.select([resu.side == 'S', resu.side == 'B'], \n",
    "                                 [-resu.price * resu.amount, resu.price * resu.amount], \n",
    "                                 default=0)\n",
    "    resu['sell_volume'] = resu[['sell_volume', 'timestamp']].rolling('1ms', on='timestamp', center=True, min_periods=1).sum().sell_volume\n",
    "    resu['buy_volume'] = resu[['buy_volume', 'timestamp']].rolling('1ms', on='timestamp', center=True, min_periods=1).sum().buy_volume\n",
    "    resu['delta_volume'] = resu[['d_volume', 'timestamp']].rolling('1ms', on='timestamp', center=True, min_periods=1).sum().d_volume\n",
    "    resu['volume'] = resu[['amount', 'timestamp']].rolling('1ms', on='timestamp', center=True, min_periods=1).sum().amount\n",
    "    resu.set_index('timestamp', drop=True, inplace=True)\n",
    "    resu = resu[~resu.index.duplicated(keep='last')]\n",
    "    resu['close_price'] = resu['price']\n",
    "    del resu['d_volume'], resu['price'], resu['amount'], resu['side']\n",
    "    suf = '_cdl_'+delta_t\n",
    "    resu = resu.add_suffix(suf)\n",
    "    return resu\n",
    "    \n",
    "    \n",
    "def ATR_cdl(data_candles: pd.DataFrame, time: str,  period: int=14, mean_period: int=15):\n",
    "    suffix = '_cdl_'+time\n",
    "    data_candles['min-close'] = abs(data_candles['low'+suffix] - data_candles['close_price'+suffix])\n",
    "    data_candles['max-close'] = abs(data_candles['high'+suffix] - data_candles['close_price'+suffix])\n",
    "    data_candles['TR'] = data_candles[['min-close', 'max-close', 'spread'+suffix]].max(axis=1)\n",
    "    ATR = [data_candles['TR'].iloc[0]]\n",
    "    ln = len(data_candles)\n",
    "    for i in range(1, ln):\n",
    "        ATR.append((ATR[i-1]*(period - 1) + data_candles['TR'].iloc[i])/period)\n",
    "    data_candles['ATR'+suffix] = ATR\n",
    "    del data_candles['TR'], data_candles['max-close'], data_candles['min-close']\n",
    "    data_candles['ATR_mean'+str(mean_period)+suffix] = data_candles['ATR'+suffix].rolling(mean_period, min_periods=1).mean()\n",
    "    data_candles['ATR_dev'+suffix] = data_candles['ATR_mean'+str(mean_period)+suffix] - data_candles['ATR'+suffix]\n",
    "    #data_candles['ATR'] = data.TR.ewm(span= delta_n * period).mean()\n",
    "\n",
    "\n",
    "    \n",
    "def EMA_cdl(data_candles: pd.DataFrame, time: str,  period: int=233):\n",
    "    suffix = str(period)+'_cdl_'+time \n",
    "    data_candles['EMA'+suffix] = data_candles['close_price'+'_cdl_'+time ].ewm(span=period).mean()\n",
    "    data_candles['EMA'+suffix+'dist'] = (data_candles['close_price'+'_cdl_'+time ] - data_candles['EMA'+suffix]) / data_candles['close_price'+'_cdl_'+time ]\n",
    "\n",
    "\n",
    "def EMA_INTCP_cdl(data_candles: pd.DataFrame, time: str,  period_1st: int=50, period_2nd: int=21):\n",
    "    suffix = '_cdl_'+time\n",
    "    EMA_cdl(data_candles, time, period=period_1st)\n",
    "    EMA_cdl(data_candles, time, period=period_2nd)\n",
    "    data_candles['EMA_intr'+suffix+str(period_1st)+'_'+str(period_2nd)] = \\\n",
    "        data_candles['EMA'+str(period_1st)+suffix] - data_candles['EMA'+str(period_2nd)+suffix]\n",
    "    data_candles['EMA_intr_rel'+suffix+str(period_1st)+'_'+str(period_2nd)] = \\\n",
    "        data_candles['EMA_intr'+suffix+str(period_1st)+'_'+str(period_2nd)]/data_candles['close_price'+suffix]\n",
    "    \n",
    "\n",
    "def Levels_cdl(data_candles: pd.DataFrame, time: str):\n",
    "    suffix = '_cdl_'+time\n",
    "    lvls = pd.DataFrame(data_candles.index)\n",
    "    lvls.set_index('timestamp', drop=True, inplace=True)\n",
    "    lvls['lvl_up'] = np.where((data_candles['high'+suffix] > data_candles['high'+suffix].shift(1)) \n",
    "                            & (data_candles['high'+suffix] > data_candles['high'+suffix].shift(-1))\n",
    "                            & (data_candles['high'+suffix].shift(1) > data_candles['high'+suffix].shift(2))\n",
    "                            & (data_candles['high'+suffix].shift(-1) > data_candles['high'+suffix].shift(-2)),\n",
    "                            data_candles['high'+suffix], np.NaN)\n",
    "    lvls['lvl_down'] = np.where((data_candles['low'+suffix] < data_candles['low'+suffix].shift(1)) \n",
    "                            & (data_candles['low'+suffix] < data_candles['low'+suffix].shift(-1))\n",
    "                            & (data_candles['low'+suffix].shift(1) < data_candles['low'+suffix].shift(2))\n",
    "                            & (data_candles['low'+suffix].shift(-1) < data_candles['low'+suffix].shift(-2)),\n",
    "                            data_candles['low'+suffix], np.NaN)\n",
    "    lvls = pd.concat([lvls['lvl_up'], lvls['lvl_down']])\n",
    "    lvls = lvls.dropna()\n",
    "    lvls = pd.DataFrame(lvls)\n",
    "    lvls = lvls.sort_index()\n",
    "    lvls = lvls.rename(columns={0: 'levels'+suffix})\n",
    "    data_lvls = pd.merge_asof(data_candles, lvls,  direction='backward', left_on='timestamp' , right_on='timestamp')\n",
    "    data_lvls['levels'+suffix] = data_lvls['levels'+suffix].fillna(data_lvls['close_price'+suffix])\n",
    "    data_candles['levels'+suffix] = data_lvls['levels'+suffix].values\n",
    "    data_candles['levels_dist'+suffix] = (data_candles['levels'+suffix] - data_candles['close_price'+suffix])/data_candles['close_price'+suffix]\n",
    "    del data_lvls, lvls\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dataset = pd.read_csv('target.csv', index_col='local_ts')\n",
    "target_dataset = target_dataset.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dataset = target_dataset.iloc[::20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#r, sf = Create_candles(target_dataset,trades_dataset, '6s' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_dataset = pd.read_csv('book.csv.gz', index_col='local_timestamp', usecols=[i for i in range(3,44)]) # , nrows=100000 для тестирования\n",
    "#ticker_dataset = pd.read_csv('ticker.csv.gz', index_col='local_ts' , nrows=1000000 ) \n",
    "trades_dataset = pd.read_csv('trades.csv.gz', index_col='local_ts', usecols=(0,4,5,6))\n",
    "book_dataset = book_dataset[~book_dataset.index.duplicated(keep='last')]\n",
    "#ticker_dataset = ticker_dataset[~ticker_dataset.index.duplicated(keep='last')]\n",
    "trades_dataset = trades_dataset[~trades_dataset.index.duplicated(keep='last')]\n",
    "book_dataset = book_dataset.sort_index()\n",
    "#ticker_dataset = ticker_dataset.sort_index()\n",
    "trades_dataset = trades_dataset.sort_index()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data to train, validation, target (5:2:3) parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_len = len(target_dataset)\n",
    "train_end = int(data_len*0.5)\n",
    "validation_end = int(data_len*0.7)\n",
    "train_target = target_dataset.iloc[:train_end]\n",
    "validation_target = target_dataset.iloc[train_end+1:validation_end]\n",
    "test_target = target_dataset.iloc[validation_end+1:]\n",
    "\n",
    "train_time_end = train_target.index.max()\n",
    "validation_time_end = validation_target.index.max()\n",
    "\n",
    "del validation_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_book_dataset = book_dataset.loc[(book_dataset.index < train_time_end)]\n",
    "#validation_book_dataset = book_dataset.loc[(book_dataset.index < validation_time_end) & (book_dataset.index >= train_time_end)]\n",
    "\n",
    "train_trades_dataset = trades_dataset.loc[(trades_dataset.index < train_time_end)]\n",
    "#validation_trades_dataset = trades_dataset.loc[(trades_dataset.index < validation_time_end) & (trades_dataset.index >= train_time_end)]\n",
    "\n",
    "del book_dataset, trades_dataset, target_dataset #, validation_book_dataset, validation_trades_dataset, validation_target\n",
    "#train_book_dataset, train_trades_dataset, train_target\n",
    "\n",
    "book_spot_dataset = pd.read_csv('book.spot.csv.gz', index_col='local_timestamp', usecols=[i for i in range(3,44)]) # , nrows=100000 для тестирования\n",
    "#ticker_spot_dataset = pd.read_csv('ticker.spot.csv.gz', index_col='local_ts' , nrows=1000000 ) \n",
    "trades_spot_dataset = pd.read_csv('trades.spot.csv.gz', index_col='local_ts')\n",
    "\n",
    "\n",
    "book_spot_dataset = book_spot_dataset[~book_spot_dataset.index.duplicated(keep='last')]\n",
    "#ticker_spot_dataset = ticker_spot_dataset[~ticker_spot_dataset.index.duplicated(keep='last')]\n",
    "trades_spot_dataset = trades_spot_dataset[~trades_spot_dataset.index.duplicated(keep='last')]\n",
    "book_spot_dataset = book_spot_dataset.sort_index()\n",
    "\n",
    "#ticker_spot_dataset = ticker_spot_dataset.sort_index()\n",
    "trades_spot_dataset = trades_spot_dataset.sort_index()\n",
    "\n",
    "train_book_spot_dataset = book_spot_dataset.loc[(book_spot_dataset.index < train_time_end)]\n",
    "#validation_book_spot_dataset = book_spot_dataset.loc[(book_spot_dataset.index < validation_time_end) & (book_spot_dataset.index >= train_time_end)]\n",
    "train_trades_spot_dataset = trades_spot_dataset.loc[(trades_spot_dataset.index < train_time_end)]\n",
    "#validation_trades_spot_dataset = trades_spot_dataset.loc[(trades_spot_dataset.index < validation_time_end) & (trades_spot_dataset.index >= train_time_end)]\n",
    "\n",
    "train_trades_dataset['old_price'] = train_trades_dataset.price\n",
    "train_trades_dataset['price'] = (train_trades_dataset['old_price'] - 10000) / (70000)\n",
    "#validation_trades_dataset['old_price'] = validation_trades_dataset.price\n",
    "#validation_trades_dataset['price'] = (validation_trades_dataset['old_price']- 10000) / (70000)\n",
    "\n",
    "train_trades_spot_dataset['old_price'] = train_trades_spot_dataset.price\n",
    "train_trades_spot_dataset['price'] = (train_trades_spot_dataset['old_price']- 10000) / (70000)\n",
    "#validation_trades_spot_dataset['old_price'] = validation_trades_spot_dataset.price\n",
    "#validation_trades_spot_dataset['price'] = (validation_trades_spot_dataset['old_price']- 10000) / (70000)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "Candle_5  = Create_candles(train_target, train_trades_dataset, '5s')\n",
    "Candle_50 = Create_candles(train_target, train_trades_dataset, '50s')\n",
    "ATR_cdl(Candle_5, '5s')\n",
    "EMA_cdl(Candle_5, '5s')\n",
    "EMA_INTCP_cdl(Candle_5, '5s')\n",
    "Levels_cdl(Candle_5, '5s')\n",
    "ATR_cdl(Candle_50, '50s')\n",
    "EMA_cdl(Candle_50, '50s')\n",
    "EMA_INTCP_cdl(Candle_50, '50s')\n",
    "Levels_cdl(Candle_50, '50s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "Divergence(train_trades_dataset, train_trades_spot_dataset)\n",
    "train_trades_dataset = pd.concat([train_trades_dataset, train_trades_spot_dataset])\n",
    "train_trades_dataset = train_trades_dataset.sort_index()\n",
    "del train_trades_spot_dataset\n",
    "\n",
    "#Divergence(validation_trades_dataset, validation_trades_spot_dataset)\n",
    "#validation_trades_dataset = pd.concat([validation_trades_dataset, validation_trades_spot_dataset])\n",
    "#validation_trades_dataset = validation_trades_dataset.sort_index()\n",
    "#del validation_trades_spot_dataset\n",
    "\n",
    "train_book_dataset = pd.concat([train_book_dataset, train_book_spot_dataset])\n",
    "train_book_dataset = train_book_dataset.sort_index()\n",
    "del train_book_spot_dataset\n",
    "\n",
    "#validation_book_dataset = pd.concat([validation_book_dataset, validation_book_spot_dataset])\n",
    "#validation_book_dataset = validation_book_dataset.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alska\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\arraylike.py:396: RuntimeWarning: invalid value encountered in sqrt\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "Order_book_spread(train_book_dataset)\n",
    "Book_sum_volume_speed(train_book_dataset)\n",
    "Naive_delays(train_book_dataset)\n",
    "\n",
    "EG = Cointegration(train_book_dataset)\n",
    "Order_book_imbalance_level(train_book_dataset)\n",
    "IMB(train_book_dataset)\n",
    "\n",
    "Autocorrelation(train_trades_dataset)\n",
    "Accumulation_Distribution_Line(train_trades_dataset, '100ms')\n",
    "Average_directional_index(train_trades_dataset, 100)\n",
    "Chande_momentum_oscillator(train_trades_dataset, 75)\n",
    "Momentum(train_trades_dataset, 500)\n",
    "Rate_of_change(train_trades_dataset, 500)\n",
    "Stochast_relative_strength_index(train_trades_dataset, 150)\n",
    "Linear_regression_line(train_trades_dataset, 350)\n",
    "Realized_Volatility(train_trades_dataset, 150)\n",
    "Gauss_Kernel(train_trades_dataset, 500)\n",
    "Autocorrelation(train_trades_dataset)\n",
    "Partial_correlation(train_trades_dataset)\n",
    "Jump_Variation(train_trades_dataset, 500)\n",
    "Trade_imbalance(train_trades_dataset, '500ms')\n",
    "past_returns(train_trades_dataset, 250)\n",
    "\n",
    "Trade_price_spread(train_trades_dataset)\n",
    "Trade_price_speed(train_trades_dataset, '10000ms')\n",
    "Trades_frequency(train_trades_dataset, '250ms')\n",
    "Trades_volume_speed(train_trades_dataset)\n",
    "Big_trades_frequency(train_trades_dataset)\n",
    "\n",
    "Delta_volume_without_trades(train_book_dataset, train_trades_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alska\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\optimize\\_linesearch.py:456: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "c:\\Users\\alska\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\optimize\\_linesearch.py:305: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "c:\\Users\\alska\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\optimize.py:203: UserWarning: Line Search failed\n",
      "  warnings.warn(\"Line Search failed\")\n"
     ]
    }
   ],
   "source": [
    "train_book_dataset.timestamp = train_book_dataset.index\n",
    "train_target_plus_book = pd.merge_asof(train_target, train_book_dataset,  direction='backward', left_on='local_ts' , right_on='local_timestamp').dropna()\n",
    "\n",
    "del train_target, train_book_dataset\n",
    "\n",
    "Logreg, level_names = Adaptive_log_regression(train_target_plus_book)\n",
    "train_target_plus_book['Logreg'] = Logreg.predict(train_target_plus_book[level_names])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target_plus_book['local_timestamp'] = train_target_plus_book.timestamp.astype(np.int64)\n",
    "del train_target_plus_book['timestamp']\n",
    "train_target_plus_book = train_target_plus_book.sort_values(by=['local_timestamp'])\n",
    "train_target_plus_book = pd.merge_asof(train_target_plus_book, train_trades_dataset,  direction='backward', left_on='local_timestamp' , right_on='local_ts')#.dropna()\n",
    "del train_trades_dataset\n",
    "train_target_plus_book = train_target_plus_book.drop_duplicates(keep='first', subset = ['asks[0].price', 'asks[0].amount', 'bids[0].price', 'bids[0].amount'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_target_plus_book['seq'], train_target_plus_book['remote_ts'],  train_target_plus_book['remote_ts2'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        2023-03-22 00:00:06.434887936\n",
       "1        2023-03-22 00:00:06.561518080\n",
       "2        2023-03-22 00:00:06.689776128\n",
       "3        2023-03-22 00:00:06.803023872\n",
       "4        2023-03-22 00:00:06.900708608\n",
       "                      ...             \n",
       "939127   2023-03-23 06:31:00.575541760\n",
       "939130   2023-03-23 06:31:00.602443459\n",
       "939131   2023-03-23 06:31:00.624221696\n",
       "939132   2023-03-23 06:31:00.624584852\n",
       "939135   2023-03-23 06:31:00.652203008\n",
       "Name: timestamp, Length: 742937, dtype: datetime64[ns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_target_plus_book['timestamp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target_plus_book = pd.merge_asof(train_target_plus_book, Candle_5,  direction='backward', left_on='timestamp' , right_on='timestamp')\n",
    "train_target_plus_book = pd.merge_asof(train_target_plus_book, Candle_50,  direction='backward', left_on='timestamp' , right_on='timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>asks[0].price</th>\n",
       "      <th>asks[0].amount</th>\n",
       "      <th>bids[0].price</th>\n",
       "      <th>bids[0].amount</th>\n",
       "      <th>asks[1].price</th>\n",
       "      <th>asks[1].amount</th>\n",
       "      <th>bids[1].price</th>\n",
       "      <th>bids[1].amount</th>\n",
       "      <th>asks[2].price</th>\n",
       "      <th>...</th>\n",
       "      <th>EMA233_cdl_50s</th>\n",
       "      <th>EMA233_cdl_50sdist</th>\n",
       "      <th>EMA50_cdl_50s</th>\n",
       "      <th>EMA50_cdl_50sdist</th>\n",
       "      <th>EMA21_cdl_50s</th>\n",
       "      <th>EMA21_cdl_50sdist</th>\n",
       "      <th>EMA_intr_cdl_50s50_21</th>\n",
       "      <th>EMA_intr_rel_cdl_50s50_21</th>\n",
       "      <th>levels_cdl_50s</th>\n",
       "      <th>levels_dist_cdl_50s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>-1</td>\n",
       "      <td>28109.44</td>\n",
       "      <td>0.19034</td>\n",
       "      <td>28109.21</td>\n",
       "      <td>0.05320</td>\n",
       "      <td>28109.65</td>\n",
       "      <td>0.24000</td>\n",
       "      <td>28108.97</td>\n",
       "      <td>0.00068</td>\n",
       "      <td>28109.76</td>\n",
       "      <td>...</td>\n",
       "      <td>0.258490</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.258490</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.258490</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.258490</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>-1</td>\n",
       "      <td>28093.40</td>\n",
       "      <td>10.86000</td>\n",
       "      <td>28093.10</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>28093.50</td>\n",
       "      <td>0.52400</td>\n",
       "      <td>28093.00</td>\n",
       "      <td>0.21300</td>\n",
       "      <td>28093.60</td>\n",
       "      <td>...</td>\n",
       "      <td>0.258490</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.258490</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.258490</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.258490</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>-1</td>\n",
       "      <td>28091.80</td>\n",
       "      <td>3.19500</td>\n",
       "      <td>28091.70</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>28091.90</td>\n",
       "      <td>6.29200</td>\n",
       "      <td>28091.60</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>28092.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.258490</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.258490</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.258490</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.258490</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>-1</td>\n",
       "      <td>28106.45</td>\n",
       "      <td>0.00564</td>\n",
       "      <td>28106.02</td>\n",
       "      <td>0.39491</td>\n",
       "      <td>28106.98</td>\n",
       "      <td>0.07342</td>\n",
       "      <td>28105.83</td>\n",
       "      <td>0.05320</td>\n",
       "      <td>28107.27</td>\n",
       "      <td>...</td>\n",
       "      <td>0.258490</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.258490</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.258490</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.258490</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>-1</td>\n",
       "      <td>28090.70</td>\n",
       "      <td>11.55400</td>\n",
       "      <td>28090.60</td>\n",
       "      <td>0.10600</td>\n",
       "      <td>28090.80</td>\n",
       "      <td>0.26500</td>\n",
       "      <td>28090.50</td>\n",
       "      <td>0.02000</td>\n",
       "      <td>28090.90</td>\n",
       "      <td>...</td>\n",
       "      <td>0.258490</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.258490</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.258490</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.258490</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>742932</th>\n",
       "      <td>1</td>\n",
       "      <td>27631.80</td>\n",
       "      <td>0.02600</td>\n",
       "      <td>27631.70</td>\n",
       "      <td>13.26900</td>\n",
       "      <td>27631.90</td>\n",
       "      <td>0.02300</td>\n",
       "      <td>27631.60</td>\n",
       "      <td>0.11500</td>\n",
       "      <td>27632.10</td>\n",
       "      <td>...</td>\n",
       "      <td>0.249169</td>\n",
       "      <td>0.010768</td>\n",
       "      <td>0.251279</td>\n",
       "      <td>0.002393</td>\n",
       "      <td>0.251702</td>\n",
       "      <td>0.000712</td>\n",
       "      <td>-0.000423</td>\n",
       "      <td>-0.001681</td>\n",
       "      <td>0.251691</td>\n",
       "      <td>-0.000754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>742933</th>\n",
       "      <td>1</td>\n",
       "      <td>27633.50</td>\n",
       "      <td>0.02600</td>\n",
       "      <td>27633.40</td>\n",
       "      <td>2.82800</td>\n",
       "      <td>27633.70</td>\n",
       "      <td>1.28100</td>\n",
       "      <td>27633.30</td>\n",
       "      <td>1.32500</td>\n",
       "      <td>27633.90</td>\n",
       "      <td>...</td>\n",
       "      <td>0.249169</td>\n",
       "      <td>0.010768</td>\n",
       "      <td>0.251279</td>\n",
       "      <td>0.002393</td>\n",
       "      <td>0.251702</td>\n",
       "      <td>0.000712</td>\n",
       "      <td>-0.000423</td>\n",
       "      <td>-0.001681</td>\n",
       "      <td>0.251691</td>\n",
       "      <td>-0.000754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>742934</th>\n",
       "      <td>1</td>\n",
       "      <td>27648.82</td>\n",
       "      <td>0.01522</td>\n",
       "      <td>27648.81</td>\n",
       "      <td>7.77154</td>\n",
       "      <td>27649.04</td>\n",
       "      <td>0.05709</td>\n",
       "      <td>27648.73</td>\n",
       "      <td>0.01602</td>\n",
       "      <td>27649.17</td>\n",
       "      <td>...</td>\n",
       "      <td>0.249169</td>\n",
       "      <td>0.010768</td>\n",
       "      <td>0.251279</td>\n",
       "      <td>0.002393</td>\n",
       "      <td>0.251702</td>\n",
       "      <td>0.000712</td>\n",
       "      <td>-0.000423</td>\n",
       "      <td>-0.001681</td>\n",
       "      <td>0.251691</td>\n",
       "      <td>-0.000754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>742935</th>\n",
       "      <td>1</td>\n",
       "      <td>27633.50</td>\n",
       "      <td>0.02100</td>\n",
       "      <td>27633.40</td>\n",
       "      <td>19.71700</td>\n",
       "      <td>27633.70</td>\n",
       "      <td>1.28000</td>\n",
       "      <td>27633.20</td>\n",
       "      <td>0.32100</td>\n",
       "      <td>27633.90</td>\n",
       "      <td>...</td>\n",
       "      <td>0.249169</td>\n",
       "      <td>0.010768</td>\n",
       "      <td>0.251279</td>\n",
       "      <td>0.002393</td>\n",
       "      <td>0.251702</td>\n",
       "      <td>0.000712</td>\n",
       "      <td>-0.000423</td>\n",
       "      <td>-0.001681</td>\n",
       "      <td>0.251691</td>\n",
       "      <td>-0.000754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>742936</th>\n",
       "      <td>1</td>\n",
       "      <td>27633.70</td>\n",
       "      <td>2.05400</td>\n",
       "      <td>27633.60</td>\n",
       "      <td>12.94300</td>\n",
       "      <td>27633.90</td>\n",
       "      <td>0.02100</td>\n",
       "      <td>27633.40</td>\n",
       "      <td>2.03600</td>\n",
       "      <td>27634.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.249169</td>\n",
       "      <td>0.010768</td>\n",
       "      <td>0.251279</td>\n",
       "      <td>0.002393</td>\n",
       "      <td>0.251702</td>\n",
       "      <td>0.000712</td>\n",
       "      <td>-0.000423</td>\n",
       "      <td>-0.001681</td>\n",
       "      <td>0.251691</td>\n",
       "      <td>-0.000754</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>742544 rows × 145 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        target  asks[0].price  asks[0].amount  bids[0].price  bids[0].amount   \n",
       "393         -1       28109.44         0.19034       28109.21         0.05320  \\\n",
       "394         -1       28093.40        10.86000       28093.10         0.00100   \n",
       "395         -1       28091.80         3.19500       28091.70         0.00100   \n",
       "396         -1       28106.45         0.00564       28106.02         0.39491   \n",
       "397         -1       28090.70        11.55400       28090.60         0.10600   \n",
       "...        ...            ...             ...            ...             ...   \n",
       "742932       1       27631.80         0.02600       27631.70        13.26900   \n",
       "742933       1       27633.50         0.02600       27633.40         2.82800   \n",
       "742934       1       27648.82         0.01522       27648.81         7.77154   \n",
       "742935       1       27633.50         0.02100       27633.40        19.71700   \n",
       "742936       1       27633.70         2.05400       27633.60        12.94300   \n",
       "\n",
       "        asks[1].price  asks[1].amount  bids[1].price  bids[1].amount   \n",
       "393          28109.65         0.24000       28108.97         0.00068  \\\n",
       "394          28093.50         0.52400       28093.00         0.21300   \n",
       "395          28091.90         6.29200       28091.60         0.00100   \n",
       "396          28106.98         0.07342       28105.83         0.05320   \n",
       "397          28090.80         0.26500       28090.50         0.02000   \n",
       "...               ...             ...            ...             ...   \n",
       "742932       27631.90         0.02300       27631.60         0.11500   \n",
       "742933       27633.70         1.28100       27633.30         1.32500   \n",
       "742934       27649.04         0.05709       27648.73         0.01602   \n",
       "742935       27633.70         1.28000       27633.20         0.32100   \n",
       "742936       27633.90         0.02100       27633.40         2.03600   \n",
       "\n",
       "        asks[2].price  ...  EMA233_cdl_50s  EMA233_cdl_50sdist  EMA50_cdl_50s   \n",
       "393          28109.76  ...        0.258490            0.000000       0.258490  \\\n",
       "394          28093.60  ...        0.258490            0.000000       0.258490   \n",
       "395          28092.00  ...        0.258490            0.000000       0.258490   \n",
       "396          28107.27  ...        0.258490            0.000000       0.258490   \n",
       "397          28090.90  ...        0.258490            0.000000       0.258490   \n",
       "...               ...  ...             ...                 ...            ...   \n",
       "742932       27632.10  ...        0.249169            0.010768       0.251279   \n",
       "742933       27633.90  ...        0.249169            0.010768       0.251279   \n",
       "742934       27649.17  ...        0.249169            0.010768       0.251279   \n",
       "742935       27633.90  ...        0.249169            0.010768       0.251279   \n",
       "742936       27634.00  ...        0.249169            0.010768       0.251279   \n",
       "\n",
       "        EMA50_cdl_50sdist  EMA21_cdl_50s  EMA21_cdl_50sdist   \n",
       "393              0.000000       0.258490           0.000000  \\\n",
       "394              0.000000       0.258490           0.000000   \n",
       "395              0.000000       0.258490           0.000000   \n",
       "396              0.000000       0.258490           0.000000   \n",
       "397              0.000000       0.258490           0.000000   \n",
       "...                   ...            ...                ...   \n",
       "742932           0.002393       0.251702           0.000712   \n",
       "742933           0.002393       0.251702           0.000712   \n",
       "742934           0.002393       0.251702           0.000712   \n",
       "742935           0.002393       0.251702           0.000712   \n",
       "742936           0.002393       0.251702           0.000712   \n",
       "\n",
       "        EMA_intr_cdl_50s50_21  EMA_intr_rel_cdl_50s50_21  levels_cdl_50s   \n",
       "393                  0.000000                   0.000000        0.258490  \\\n",
       "394                  0.000000                   0.000000        0.258490   \n",
       "395                  0.000000                   0.000000        0.258490   \n",
       "396                  0.000000                   0.000000        0.258490   \n",
       "397                  0.000000                   0.000000        0.258490   \n",
       "...                       ...                        ...             ...   \n",
       "742932              -0.000423                  -0.001681        0.251691   \n",
       "742933              -0.000423                  -0.001681        0.251691   \n",
       "742934              -0.000423                  -0.001681        0.251691   \n",
       "742935              -0.000423                  -0.001681        0.251691   \n",
       "742936              -0.000423                  -0.001681        0.251691   \n",
       "\n",
       "        levels_dist_cdl_50s  \n",
       "393                0.000000  \n",
       "394                0.000000  \n",
       "395                0.000000  \n",
       "396                0.000000  \n",
       "397                0.000000  \n",
       "...                     ...  \n",
       "742932            -0.000754  \n",
       "742933            -0.000754  \n",
       "742934            -0.000754  \n",
       "742935            -0.000754  \n",
       "742936            -0.000754  \n",
       "\n",
       "[742544 rows x 145 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_target_plus_book"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target_plus_book.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "train_target_plus_book = train_target_plus_book.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_use = [i for i in train_target_plus_book.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in ['local_timestamp','side','timestamp', 'target']:\n",
    "    features_to_use.remove(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_scale = scaler.fit_transform(train_target_plus_book[features_to_use].iloc[::10]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asks[0].price</th>\n",
       "      <th>asks[0].amount</th>\n",
       "      <th>bids[0].price</th>\n",
       "      <th>bids[0].amount</th>\n",
       "      <th>asks[1].price</th>\n",
       "      <th>asks[1].amount</th>\n",
       "      <th>bids[1].price</th>\n",
       "      <th>bids[1].amount</th>\n",
       "      <th>asks[2].price</th>\n",
       "      <th>asks[2].amount</th>\n",
       "      <th>...</th>\n",
       "      <th>EMA233_cdl_50s</th>\n",
       "      <th>EMA233_cdl_50sdist</th>\n",
       "      <th>EMA50_cdl_50s</th>\n",
       "      <th>EMA50_cdl_50sdist</th>\n",
       "      <th>EMA21_cdl_50s</th>\n",
       "      <th>EMA21_cdl_50sdist</th>\n",
       "      <th>EMA_intr_cdl_50s50_21</th>\n",
       "      <th>EMA_intr_rel_cdl_50s50_21</th>\n",
       "      <th>levels_cdl_50s</th>\n",
       "      <th>levels_dist_cdl_50s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>28109.44</td>\n",
       "      <td>0.19034</td>\n",
       "      <td>28109.21</td>\n",
       "      <td>0.05320</td>\n",
       "      <td>28109.65</td>\n",
       "      <td>0.24000</td>\n",
       "      <td>28108.97</td>\n",
       "      <td>0.00068</td>\n",
       "      <td>28109.76</td>\n",
       "      <td>0.00070</td>\n",
       "      <td>...</td>\n",
       "      <td>0.258490</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.258490</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.258490</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.258490</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>28106.45</td>\n",
       "      <td>0.00564</td>\n",
       "      <td>28106.02</td>\n",
       "      <td>0.39491</td>\n",
       "      <td>28106.98</td>\n",
       "      <td>0.07342</td>\n",
       "      <td>28105.83</td>\n",
       "      <td>0.05320</td>\n",
       "      <td>28107.27</td>\n",
       "      <td>0.00065</td>\n",
       "      <td>...</td>\n",
       "      <td>0.258490</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.258490</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.258490</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.258490</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>28107.13</td>\n",
       "      <td>0.00266</td>\n",
       "      <td>28106.04</td>\n",
       "      <td>0.01221</td>\n",
       "      <td>28107.14</td>\n",
       "      <td>0.53429</td>\n",
       "      <td>28106.03</td>\n",
       "      <td>0.00300</td>\n",
       "      <td>28107.56</td>\n",
       "      <td>0.00074</td>\n",
       "      <td>...</td>\n",
       "      <td>0.258490</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.258490</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.258490</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.258490</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>28105.12</td>\n",
       "      <td>0.24000</td>\n",
       "      <td>28104.87</td>\n",
       "      <td>0.02000</td>\n",
       "      <td>28105.42</td>\n",
       "      <td>0.19868</td>\n",
       "      <td>28104.84</td>\n",
       "      <td>0.00700</td>\n",
       "      <td>28105.47</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.258490</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.258490</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.258490</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.258490</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405</th>\n",
       "      <td>28104.88</td>\n",
       "      <td>0.19048</td>\n",
       "      <td>28104.57</td>\n",
       "      <td>0.02799</td>\n",
       "      <td>28104.89</td>\n",
       "      <td>0.02866</td>\n",
       "      <td>28104.44</td>\n",
       "      <td>0.10029</td>\n",
       "      <td>28105.13</td>\n",
       "      <td>0.22689</td>\n",
       "      <td>...</td>\n",
       "      <td>0.258490</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.258490</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.258490</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.258490</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>742923</th>\n",
       "      <td>27631.80</td>\n",
       "      <td>6.13600</td>\n",
       "      <td>27631.70</td>\n",
       "      <td>8.43300</td>\n",
       "      <td>27631.90</td>\n",
       "      <td>0.02500</td>\n",
       "      <td>27631.60</td>\n",
       "      <td>0.09000</td>\n",
       "      <td>27632.10</td>\n",
       "      <td>0.01900</td>\n",
       "      <td>...</td>\n",
       "      <td>0.249169</td>\n",
       "      <td>0.010768</td>\n",
       "      <td>0.251279</td>\n",
       "      <td>0.002393</td>\n",
       "      <td>0.251702</td>\n",
       "      <td>0.000712</td>\n",
       "      <td>-0.000423</td>\n",
       "      <td>-0.001681</td>\n",
       "      <td>0.251691</td>\n",
       "      <td>-0.000754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>742926</th>\n",
       "      <td>27647.80</td>\n",
       "      <td>0.28481</td>\n",
       "      <td>27647.79</td>\n",
       "      <td>8.53559</td>\n",
       "      <td>27647.81</td>\n",
       "      <td>0.07826</td>\n",
       "      <td>27647.73</td>\n",
       "      <td>0.00322</td>\n",
       "      <td>27648.03</td>\n",
       "      <td>0.00187</td>\n",
       "      <td>...</td>\n",
       "      <td>0.249169</td>\n",
       "      <td>0.010768</td>\n",
       "      <td>0.251279</td>\n",
       "      <td>0.002393</td>\n",
       "      <td>0.251702</td>\n",
       "      <td>0.000712</td>\n",
       "      <td>-0.000423</td>\n",
       "      <td>-0.001681</td>\n",
       "      <td>0.251691</td>\n",
       "      <td>-0.000754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>742929</th>\n",
       "      <td>27631.80</td>\n",
       "      <td>3.90300</td>\n",
       "      <td>27631.70</td>\n",
       "      <td>10.85100</td>\n",
       "      <td>27631.90</td>\n",
       "      <td>0.02300</td>\n",
       "      <td>27631.60</td>\n",
       "      <td>0.10200</td>\n",
       "      <td>27632.10</td>\n",
       "      <td>0.01900</td>\n",
       "      <td>...</td>\n",
       "      <td>0.249169</td>\n",
       "      <td>0.010768</td>\n",
       "      <td>0.251279</td>\n",
       "      <td>0.002393</td>\n",
       "      <td>0.251702</td>\n",
       "      <td>0.000712</td>\n",
       "      <td>-0.000423</td>\n",
       "      <td>-0.001681</td>\n",
       "      <td>0.251691</td>\n",
       "      <td>-0.000754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>742932</th>\n",
       "      <td>27631.80</td>\n",
       "      <td>0.02600</td>\n",
       "      <td>27631.70</td>\n",
       "      <td>13.26900</td>\n",
       "      <td>27631.90</td>\n",
       "      <td>0.02300</td>\n",
       "      <td>27631.60</td>\n",
       "      <td>0.11500</td>\n",
       "      <td>27632.10</td>\n",
       "      <td>0.01900</td>\n",
       "      <td>...</td>\n",
       "      <td>0.249169</td>\n",
       "      <td>0.010768</td>\n",
       "      <td>0.251279</td>\n",
       "      <td>0.002393</td>\n",
       "      <td>0.251702</td>\n",
       "      <td>0.000712</td>\n",
       "      <td>-0.000423</td>\n",
       "      <td>-0.001681</td>\n",
       "      <td>0.251691</td>\n",
       "      <td>-0.000754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>742935</th>\n",
       "      <td>27633.50</td>\n",
       "      <td>0.02100</td>\n",
       "      <td>27633.40</td>\n",
       "      <td>19.71700</td>\n",
       "      <td>27633.70</td>\n",
       "      <td>1.28000</td>\n",
       "      <td>27633.20</td>\n",
       "      <td>0.32100</td>\n",
       "      <td>27633.90</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>...</td>\n",
       "      <td>0.249169</td>\n",
       "      <td>0.010768</td>\n",
       "      <td>0.251279</td>\n",
       "      <td>0.002393</td>\n",
       "      <td>0.251702</td>\n",
       "      <td>0.000712</td>\n",
       "      <td>-0.000423</td>\n",
       "      <td>-0.001681</td>\n",
       "      <td>0.251691</td>\n",
       "      <td>-0.000754</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>247515 rows × 141 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        asks[0].price  asks[0].amount  bids[0].price  bids[0].amount   \n",
       "393          28109.44         0.19034       28109.21         0.05320  \\\n",
       "396          28106.45         0.00564       28106.02         0.39491   \n",
       "399          28107.13         0.00266       28106.04         0.01221   \n",
       "402          28105.12         0.24000       28104.87         0.02000   \n",
       "405          28104.88         0.19048       28104.57         0.02799   \n",
       "...               ...             ...            ...             ...   \n",
       "742923       27631.80         6.13600       27631.70         8.43300   \n",
       "742926       27647.80         0.28481       27647.79         8.53559   \n",
       "742929       27631.80         3.90300       27631.70        10.85100   \n",
       "742932       27631.80         0.02600       27631.70        13.26900   \n",
       "742935       27633.50         0.02100       27633.40        19.71700   \n",
       "\n",
       "        asks[1].price  asks[1].amount  bids[1].price  bids[1].amount   \n",
       "393          28109.65         0.24000       28108.97         0.00068  \\\n",
       "396          28106.98         0.07342       28105.83         0.05320   \n",
       "399          28107.14         0.53429       28106.03         0.00300   \n",
       "402          28105.42         0.19868       28104.84         0.00700   \n",
       "405          28104.89         0.02866       28104.44         0.10029   \n",
       "...               ...             ...            ...             ...   \n",
       "742923       27631.90         0.02500       27631.60         0.09000   \n",
       "742926       27647.81         0.07826       27647.73         0.00322   \n",
       "742929       27631.90         0.02300       27631.60         0.10200   \n",
       "742932       27631.90         0.02300       27631.60         0.11500   \n",
       "742935       27633.70         1.28000       27633.20         0.32100   \n",
       "\n",
       "        asks[2].price  asks[2].amount  ...  EMA233_cdl_50s   \n",
       "393          28109.76         0.00070  ...        0.258490  \\\n",
       "396          28107.27         0.00065  ...        0.258490   \n",
       "399          28107.56         0.00074  ...        0.258490   \n",
       "402          28105.47         0.00001  ...        0.258490   \n",
       "405          28105.13         0.22689  ...        0.258490   \n",
       "...               ...             ...  ...             ...   \n",
       "742923       27632.10         0.01900  ...        0.249169   \n",
       "742926       27648.03         0.00187  ...        0.249169   \n",
       "742929       27632.10         0.01900  ...        0.249169   \n",
       "742932       27632.10         0.01900  ...        0.249169   \n",
       "742935       27633.90         0.00100  ...        0.249169   \n",
       "\n",
       "        EMA233_cdl_50sdist  EMA50_cdl_50s  EMA50_cdl_50sdist  EMA21_cdl_50s   \n",
       "393               0.000000       0.258490           0.000000       0.258490  \\\n",
       "396               0.000000       0.258490           0.000000       0.258490   \n",
       "399               0.000000       0.258490           0.000000       0.258490   \n",
       "402               0.000000       0.258490           0.000000       0.258490   \n",
       "405               0.000000       0.258490           0.000000       0.258490   \n",
       "...                    ...            ...                ...            ...   \n",
       "742923            0.010768       0.251279           0.002393       0.251702   \n",
       "742926            0.010768       0.251279           0.002393       0.251702   \n",
       "742929            0.010768       0.251279           0.002393       0.251702   \n",
       "742932            0.010768       0.251279           0.002393       0.251702   \n",
       "742935            0.010768       0.251279           0.002393       0.251702   \n",
       "\n",
       "        EMA21_cdl_50sdist  EMA_intr_cdl_50s50_21  EMA_intr_rel_cdl_50s50_21   \n",
       "393              0.000000               0.000000                   0.000000  \\\n",
       "396              0.000000               0.000000                   0.000000   \n",
       "399              0.000000               0.000000                   0.000000   \n",
       "402              0.000000               0.000000                   0.000000   \n",
       "405              0.000000               0.000000                   0.000000   \n",
       "...                   ...                    ...                        ...   \n",
       "742923           0.000712              -0.000423                  -0.001681   \n",
       "742926           0.000712              -0.000423                  -0.001681   \n",
       "742929           0.000712              -0.000423                  -0.001681   \n",
       "742932           0.000712              -0.000423                  -0.001681   \n",
       "742935           0.000712              -0.000423                  -0.001681   \n",
       "\n",
       "        levels_cdl_50s  levels_dist_cdl_50s  \n",
       "393           0.258490             0.000000  \n",
       "396           0.258490             0.000000  \n",
       "399           0.258490             0.000000  \n",
       "402           0.258490             0.000000  \n",
       "405           0.258490             0.000000  \n",
       "...                ...                  ...  \n",
       "742923        0.251691            -0.000754  \n",
       "742926        0.251691            -0.000754  \n",
       "742929        0.251691            -0.000754  \n",
       "742932        0.251691            -0.000754  \n",
       "742935        0.251691            -0.000754  \n",
       "\n",
       "[247515 rows x 141 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_target_plus_book[features_to_use].iloc[::10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alska\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py:1111: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(multi_class=&#x27;multinomial&#x27;, solver=&#x27;newton-cg&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(multi_class=&#x27;multinomial&#x27;, solver=&#x27;newton-cg&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(multi_class='multinomial', solver='newton-cg')"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression(solver='newton-cg',  multi_class='multinomial')\n",
    "model.fit(X_scale, train_target_plus_book[['target']].iloc[::10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_target_plus_book"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_dataset = pd.read_csv('book.csv.gz', index_col='local_timestamp', usecols=[i for i in range(3,44)]) # , nrows=100000 для тестирования\n",
    "#ticker_dataset = pd.read_csv('ticker.csv.gz', index_col='local_ts' , nrows=1000000 ) \n",
    "trades_dataset = pd.read_csv('trades.csv.gz', index_col='local_ts')\n",
    "book_dataset = book_dataset[~book_dataset.index.duplicated(keep='last')]\n",
    "#ticker_dataset = ticker_dataset[~ticker_dataset.index.duplicated(keep='last')]\n",
    "trades_dataset = trades_dataset[~trades_dataset.index.duplicated(keep='last')]\n",
    "book_dataset = book_dataset.sort_index()\n",
    "#ticker_dataset = ticker_dataset.sort_index()\n",
    "trades_dataset = trades_dataset.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_book_dataset = book_dataset.loc[(book_dataset.index >= validation_time_end)]\n",
    "test_trades_dataset = trades_dataset.loc[(trades_dataset.index >= validation_time_end)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "del book_dataset, trades_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_spot_dataset = pd.read_csv('book.spot.csv.gz', index_col='local_timestamp', usecols=[i for i in range(3,44)]) # , nrows=100000 для тестирования\n",
    "#ticker_spot_dataset = pd.read_csv('ticker.spot.csv.gz', index_col='local_ts' , nrows=1000000 ) \n",
    "trades_spot_dataset = pd.read_csv('trades.spot.csv.gz', index_col='local_ts')\n",
    "book_spot_dataset = book_spot_dataset[~book_spot_dataset.index.duplicated(keep='last')]\n",
    "#ticker_spot_dataset = ticker_spot_dataset[~ticker_spot_dataset.index.duplicated(keep='last')]\n",
    "trades_spot_dataset = trades_spot_dataset[~trades_spot_dataset.index.duplicated(keep='last')]\n",
    "book_spot_dataset = book_spot_dataset.sort_index()\n",
    "#ticker_spot_dataset = ticker_spot_dataset.sort_index()\n",
    "trades_spot_dataset = trades_spot_dataset.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_book_spot_dataset = book_spot_dataset.loc[(book_spot_dataset.index >= validation_time_end)]\n",
    "test_trades_spot_dataset = trades_spot_dataset.loc[(trades_spot_dataset.index >= validation_time_end)]\n",
    "\n",
    "test_trades_spot_dataset['old_price'] = test_trades_spot_dataset.price\n",
    "test_trades_spot_dataset['price'] = (test_trades_spot_dataset['old_price']- 10000) / (70000)\n",
    "\n",
    "#del trades_spot_dataset, book_spot_dataset\n",
    "\n",
    "test_trades_dataset['old_price'] = test_trades_dataset.price\n",
    "test_trades_dataset['price'] = (test_trades_dataset['old_price'] - 10000) / (70000)\n",
    "\n",
    "test_book_dataset = pd.concat([test_book_dataset, test_book_spot_dataset])\n",
    "test_book_dataset = test_book_dataset.sort_index()\n",
    "#del test_book_spot_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "Candle_5_test  = Create_candles(test_target, test_trades_dataset, '5s')\n",
    "Candle_50_test= Create_candles(test_target, test_trades_dataset, '50s')\n",
    "ATR_cdl(Candle_5_test, '5s')\n",
    "EMA_cdl(Candle_5_test, '5s')\n",
    "EMA_INTCP_cdl(Candle_5_test, '5s')\n",
    "Levels_cdl(Candle_5_test, '5s')\n",
    "ATR_cdl(Candle_50_test, '50s')\n",
    "EMA_cdl(Candle_50_test, '50s')\n",
    "EMA_INTCP_cdl(Candle_50_test, '50s')\n",
    "Levels_cdl(Candle_50_test, '50s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "Divergence(test_trades_dataset, test_trades_spot_dataset)\n",
    "test_trades_dataset = pd.concat([test_trades_dataset, test_trades_spot_dataset])\n",
    "test_trades_dataset = test_trades_dataset.sort_index()\n",
    "del test_trades_spot_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alska\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\arraylike.py:396: RuntimeWarning: invalid value encountered in sqrt\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "Order_book_spread(test_book_dataset)\n",
    "Book_sum_volume_speed(test_book_dataset)\n",
    "Naive_delays(test_book_dataset)\n",
    "\n",
    "Cointegration_from_EG(test_book_dataset, EG)\n",
    "Order_book_imbalance_level(test_book_dataset)\n",
    "IMB(test_book_dataset)\n",
    "\n",
    "Autocorrelation(test_trades_dataset)\n",
    "Accumulation_Distribution_Line(test_trades_dataset, '100ms')\n",
    "Average_directional_index(test_trades_dataset, 100)\n",
    "Chande_momentum_oscillator(test_trades_dataset, 75)\n",
    "Momentum(test_trades_dataset, 500)\n",
    "Rate_of_change(test_trades_dataset, 500)\n",
    "Stochast_relative_strength_index(test_trades_dataset, 150)\n",
    "Linear_regression_line(test_trades_dataset, 350)\n",
    "Realized_Volatility(test_trades_dataset, 150)\n",
    "Gauss_Kernel(test_trades_dataset, 500)\n",
    "Autocorrelation(test_trades_dataset)\n",
    "Partial_correlation(test_trades_dataset)\n",
    "Jump_Variation(test_trades_dataset, 500)\n",
    "Trade_imbalance(test_trades_dataset, '500ms')\n",
    "past_returns(test_trades_dataset, 250)\n",
    "\n",
    "Trade_price_spread(test_trades_dataset)\n",
    "Trade_price_speed(test_trades_dataset, '10000ms')\n",
    "Trades_frequency(test_trades_dataset, '250ms')\n",
    "Trades_volume_speed(test_trades_dataset)\n",
    "Big_trades_frequency(test_trades_dataset)\n",
    "\n",
    "Delta_volume_without_trades(test_book_dataset, test_trades_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_book_dataset['Logreg'] = Logreg.predict(test_book_dataset[level_names])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_target['timestamp'] = test_target.index.astype(np.int64)\n",
    "test_target_plus_book = pd.merge_asof(test_target, test_book_dataset,  direction='backward', left_on='local_ts' , right_on='local_timestamp').dropna()\n",
    "##del  test_book_dataset, #test_target\n",
    "test_target_plus_book = test_target_plus_book.set_index('timestamp_x')\n",
    "test_target_plus_book = test_target_plus_book.sort_index()\n",
    "test_target_features = pd.merge_asof(test_target_plus_book, test_trades_dataset,  direction='backward', left_on='timestamp_x' , right_on='local_ts')#.dropna()\n",
    "#del test_target_plus_book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_target_features = pd.merge_asof(test_target_features, Candle_5_test,  direction='backward', left_on='timestamp' , right_on='timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_target_features = pd.merge_asof(test_target_features, Candle_50_test,  direction='backward', left_on='timestamp' , right_on='timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_target_features = test_target_features.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [i for i in test_target_features.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scale_test = scaler.fit_transform(test_target_features[features_to_use]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_target_test = model.predict(X_scale_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "210.71139500408572"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric(y_true=test_target_features['target'], y_pred=predicted_target_test )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target_plus_book['target_XGBC'] = np.where(train_target_plus_book['target'] == -1, 2,train_target_plus_book['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
       "              objective=&#x27;multi:softprob&#x27;, predictor=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
       "              objective=&#x27;multi:softprob&#x27;, predictor=None, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
       "              objective='multi:softprob', predictor=None, ...)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_clf = XGBClassifier()\n",
    "xgb_clf.fit(X_scale, train_target_plus_book['target_XGBC'].iloc[::10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0091322 , 0.01448726, 0.02004259, 0.03243427, 0.02031382,\n",
       "       0.00606479, 0.02758118, 0.00668239, 0.00349207, 0.02284849,\n",
       "       0.00881061, 0.00228742, 0.00282956, 0.0339332 , 0.0146613 ,\n",
       "       0.01197764, 0.01101955, 0.01545475, 0.01679076, 0.00476971,\n",
       "       0.00470033, 0.00409855, 0.00360795, 0.00256541, 0.        ,\n",
       "       0.02054603, 0.00092013, 0.        , 0.04036542, 0.004103  ,\n",
       "       0.00546963, 0.0064386 , 0.01423212, 0.0175676 , 0.02941175,\n",
       "       0.02950813, 0.0100111 , 0.01168927, 0.00657171, 0.01406173,\n",
       "       0.04818193, 0.01017771, 0.02938008, 0.00870763, 0.00801807,\n",
       "       0.0450674 , 0.05704783, 0.0194346 , 0.02836399, 0.03050713,\n",
       "       0.00204966, 0.0082793 , 0.01826847, 0.00164995, 0.01151221,\n",
       "       0.00962588, 0.07677917, 0.00580664, 0.00437863, 0.01806756,\n",
       "       0.01641922, 0.00235003, 0.00898653, 0.01070829, 0.00875008],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_clf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_target_test = xgb_clf.predict(X_scale_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = pd.DataFrame(predicted_target_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred['prediction'] = np.where(predicted_target_test== 2, -1, predicted_target_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "prediction\n",
       " 1    231180\n",
       " 0    217602\n",
       "-1    114679\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred['prediction'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78.88331073493447"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric(y_true=test_target_features['target'], y_pred=pred['prediction'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alska\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:98: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\alska\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LGBMClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LGBMClassifier</label><div class=\"sk-toggleable__content\"><pre>LGBMClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LGBMClassifier()"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LGBMC = LGBMClassifier()\n",
    "LGBMC.fit(X_scale,  train_target_plus_book[['target']].iloc[::10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_pred = LGBMC.predict(X_scale_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "139.44925606400585"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric(y_true=test_target_features['target'], y_pred=L_pred )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(74255, 141)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_scale.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = X_scale.reshape(X_scale.shape[0],X_scale.shape[1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "TimeSteps=X_data.shape[1]\n",
    "TotalFeatures=X_data.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_d = train_target_plus_book[['target']].iloc[::10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_d = np.select([y_d == -1, y_d == 0, y_d == 1,], \n",
    "          [ 0, 1, 2], \n",
    "           default=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = Sequential()\n",
    "regressor.add(LSTM(units = 10, activation = 'relu', input_shape = (TimeSteps, TotalFeatures), return_sequences=True))\n",
    "regressor.add(LSTM(units = 5, activation = 'relu', input_shape = (TimeSteps, TotalFeatures), return_sequences=True))\n",
    "regressor.add(LSTM(units = 5, activation = 'relu', return_sequences=False ))\n",
    "regressor.add(Dense(3, activation='softmax'))\n",
    "\n",
    "regressor.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "149/149 [==============================] - 27s 161ms/step - loss: 1.0356 - accuracy: 0.4897\n",
      "Epoch 2/30\n",
      "149/149 [==============================] - 24s 158ms/step - loss: 0.9856 - accuracy: 0.5203\n",
      "Epoch 3/30\n",
      "149/149 [==============================] - 23s 157ms/step - loss: 0.9780 - accuracy: 0.5261\n",
      "Epoch 4/30\n",
      "149/149 [==============================] - 24s 162ms/step - loss: 0.9728 - accuracy: 0.5275\n",
      "Epoch 5/30\n",
      "149/149 [==============================] - 23s 157ms/step - loss: 0.9699 - accuracy: 0.5323\n",
      "Epoch 6/30\n",
      "149/149 [==============================] - 24s 158ms/step - loss: 0.9675 - accuracy: 0.5334\n",
      "Epoch 7/30\n",
      "149/149 [==============================] - 24s 160ms/step - loss: 0.9636 - accuracy: 0.5381\n",
      "Epoch 8/30\n",
      "149/149 [==============================] - 24s 158ms/step - loss: 2.0135 - accuracy: 0.5299\n",
      "Epoch 9/30\n",
      "149/149 [==============================] - 23s 157ms/step - loss: 0.9679 - accuracy: 0.5329\n",
      "Epoch 10/30\n",
      "149/149 [==============================] - 24s 158ms/step - loss: 0.9661 - accuracy: 0.5346\n",
      "Epoch 11/30\n",
      "149/149 [==============================] - 25s 165ms/step - loss: 0.9648 - accuracy: 0.5351\n",
      "Epoch 12/30\n",
      "149/149 [==============================] - 24s 158ms/step - loss: 0.9640 - accuracy: 0.5349\n",
      "Epoch 13/30\n",
      "149/149 [==============================] - 23s 157ms/step - loss: 0.9630 - accuracy: 0.5359\n",
      "Epoch 14/30\n",
      "149/149 [==============================] - 23s 158ms/step - loss: 0.9628 - accuracy: 0.5367\n",
      "Epoch 15/30\n",
      "149/149 [==============================] - 23s 158ms/step - loss: 0.9620 - accuracy: 0.5368\n",
      "Epoch 16/30\n",
      "149/149 [==============================] - 23s 158ms/step - loss: 0.9611 - accuracy: 0.5372\n",
      "Epoch 17/30\n",
      "149/149 [==============================] - 24s 158ms/step - loss: 0.9605 - accuracy: 0.5372\n",
      "Epoch 18/30\n",
      "149/149 [==============================] - 26s 173ms/step - loss: 0.9599 - accuracy: 0.5373\n",
      "Epoch 19/30\n",
      "149/149 [==============================] - 24s 162ms/step - loss: 0.9599 - accuracy: 0.5376\n",
      "Epoch 20/30\n",
      "149/149 [==============================] - 25s 166ms/step - loss: 0.9588 - accuracy: 0.5380\n",
      "Epoch 21/30\n",
      "149/149 [==============================] - 26s 176ms/step - loss: 0.9583 - accuracy: 0.5378\n",
      "Epoch 22/30\n",
      "149/149 [==============================] - 24s 161ms/step - loss: 0.9573 - accuracy: 0.5383\n",
      "Epoch 23/30\n",
      "149/149 [==============================] - 24s 164ms/step - loss: 0.9562 - accuracy: 0.5385\n",
      "Epoch 24/30\n",
      "149/149 [==============================] - 23s 156ms/step - loss: 0.9567 - accuracy: 0.5391\n",
      "Epoch 25/30\n",
      "149/149 [==============================] - 23s 155ms/step - loss: 0.9553 - accuracy: 0.5397\n",
      "Epoch 26/30\n",
      "149/149 [==============================] - 23s 154ms/step - loss: 0.9542 - accuracy: 0.5392\n",
      "Epoch 27/30\n",
      "149/149 [==============================] - 23s 157ms/step - loss: 0.9531 - accuracy: 0.5417\n",
      "Epoch 28/30\n",
      "149/149 [==============================] - 23s 155ms/step - loss: 0.9526 - accuracy: 0.5421\n",
      "Epoch 29/30\n",
      "149/149 [==============================] - 23s 154ms/step - loss: 0.9518 - accuracy: 0.5421\n",
      "Epoch 30/30\n",
      "149/149 [==============================] - 23s 155ms/step - loss: 0.9506 - accuracy: 0.5433\n",
      "## Total Time Taken:  12 Minutes ##\n"
     ]
    }
   ],
   "source": [
    "StartTime=time.time()\n",
    "regressor.fit(X_data, y_d, batch_size = 300, epochs = 30)\n",
    "EndTime=time.time()\n",
    "print(\"## Total Time Taken: \", round((EndTime-StartTime)/60), 'Minutes ##')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17609/17609 [==============================] - 325s 18ms/step\n"
     ]
    }
   ],
   "source": [
    "pr = regressor.predict(X_scale_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.1736963 , 0.6184458 , 0.20785786],\n",
       "       [0.16969083, 0.62888676, 0.2014224 ],\n",
       "       [0.149612  , 0.68127537, 0.16911258],\n",
       "       ...,\n",
       "       [0.40931955, 0.22292131, 0.36775914],\n",
       "       [0.39851695, 0.24517979, 0.35630327],\n",
       "       [0.38465923, 0.27222317, 0.34311756]], dtype=float32)"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.1736963 , 0.16969083, 0.149612  , ..., 0.40931955, 0.39851695,\n",
       "       0.38465923], dtype=float32)"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuropredict = pd.DataFrame({'minus': pr[:,0], 'zero': pr[:,1], 'plus': pr[:,2]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuropredict['predict'] = np.select([neuropredict.minus > 0.33, neuropredict.plus > 0.33], \n",
    "                                    [ -1, 1], default=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-69.5862494034601"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric(y_true=test_target_features['target'], y_pred=neuropredict['predict'] )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c8be88799e4e7977f60998f2522923f1f9268b787ed3a9315cdbced7a5f47631"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
